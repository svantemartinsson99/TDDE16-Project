{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NOVYzkiYqjk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Drive and load dataset"
      ],
      "metadata": {
        "id": "EAgyuHy1y0-_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grHwxPbwpGB8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"drive/MyDrive/LiU/TDDE16/Project\""
      ],
      "metadata": {
        "id": "qtPYUdYEyWkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"drive/MyDrive/LiU/TDDE16/Project/The-Office-Lines-V4.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "_3U93a9bzR4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "line_lens = [len(line) for line in df[\"line\"]]\n",
        "sum(line_lens)/len(line_lens)"
      ],
      "metadata": {
        "id": "_d4hRp3Xw6of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"speaker\"].value_counts()[:10].plot(kind=\"bar\")"
      ],
      "metadata": {
        "id": "upOEd2i1TCZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports & Constants"
      ],
      "metadata": {
        "id": "L5gffbA4bc0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, cohen_kappa_score\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.dummy import DummyClassifier\n",
        "import pickle"
      ],
      "metadata": {
        "id": "S6jgFhPObbeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "RAND_STATE = 42\n",
        "random.seed(RAND_STATE)\n",
        "np.random.seed(seed=RAND_STATE)"
      ],
      "metadata": {
        "id": "bsKopYhPixgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filter relevant columns and rows"
      ],
      "metadata": {
        "id": "I4imSziSzttC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick relevant columns\n",
        "df = df[[\"line\", \"speaker\"]]\n",
        "\n",
        "# Find top 4 characters\n",
        "top_4_speakers = df[\"speaker\"].value_counts()[:4]\n",
        "print(\"The 4 characters with the most lines are:\\n\", top_4_speakers)\n",
        "top_4_speakers.plot(kind=\"bar\")\n",
        "df = df[df[\"speaker\"].isin(top_4_speakers.index)]\n",
        "print(\"The only speakers after filtering: \", df[\"speaker\"].unique())\n"
      ],
      "metadata": {
        "id": "0Rqa5umnzq8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Remove lines which contains 5 or less words."
      ],
      "metadata": {
        "id": "8zYUeerr_Rwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2F9sqdiKoPk",
        "outputId": "b48868e5-1933-484a-89fc-8ae67567440d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28720, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df[\"line\"].apply(lambda x: len(x.split()) > 5)]\n"
      ],
      "metadata": {
        "id": "gPUOvcSZ8qpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMIYDnsLKp_S",
        "outputId": "4112ea93-83b3-41c2-8b07-626376c73e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16418, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "df_michael = df[df[\"speaker\"] == \"Michael\"]\n",
        "#print(df_michael[\"line\"].head(10))\n",
        "michael_counter = Counter(\" \".join(df_michael[\"line\"].str.lower()).split()).most_common(10000)\n",
        "\n",
        "\n",
        "df_dwight = df[df[\"speaker\"] == \"Dwight\"]\n",
        "#print(df_dwight[\"line\"].head(10))\n",
        "dwight_counter = Counter(\" \".join(df_dwight[\"line\"].str.lower()).split()).most_common(3000)\n",
        "\n",
        "df_jim = df[df[\"speaker\"] == \"Jim\"]\n",
        "df_pam = df[df[\"speaker\"] == \"Pam\"]\n",
        "\n",
        "jim_counter = Counter(\" \".join(df_jim[\"line\"].str.lower()).split()).most_common(3000)\n",
        "pam_counter = Counter(\" \".join(df_pam[\"line\"].str.lower()).split()).most_common(3000)\n",
        "\n",
        "print([item for item in jim_counter if item[0] == \"fax\"])\n",
        "print([item for item in pam_counter if item[0] == \"fax\"])\n",
        "print([item for item in michael_counter if item[0] == \"fax\"])\n",
        "print([item for item in dwight_counter if item[0] == \"fax\"])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3qkoEmJeAAyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split data into train and test datasets"
      ],
      "metadata": {
        "id": "AipNHQV77pPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into train and test dataset\n",
        "x_train, x_test, y_train, y_test = train_test_split(df[\"line\"], df[\"speaker\"], test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "icWGOB0P2DMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "I5Tttqtx22ho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation function"
      ],
      "metadata": {
        "id": "UifsBttSq2uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_results(preds, true_vals):\n",
        "  print(classification_report(true_vals, preds))\n",
        "  print(\"Cohen kappa score = \", cohen_kappa_score(true_vals, preds))"
      ],
      "metadata": {
        "id": "PjC6ZmU0q1ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Downsampling"
      ],
      "metadata": {
        "id": "ZYpmKKl9BdAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "rus = RandomUnderSampler(random_state=RAND_STATE)\n",
        "\n",
        "x_train_downsampled, y_train_downsampled = rus.fit_resample(np.asarray(x_train).reshape(-1, 1), y_train)\n",
        "x_train_downsampled = x_train_downsampled.squeeze()\n",
        "\n",
        "# Shuffle the rows, since the downsampling makes them sorted on class\n",
        "df_train_downsampled = pd.DataFrame(columns=[\"line\", \"speaker\"])\n",
        "df_train_downsampled[\"line\"] = x_train_downsampled\n",
        "df_train_downsampled[\"speaker\"] = y_train_downsampled\n",
        "df_train_downsampled = df_train_downsampled.sample(frac=1)\n",
        "\n",
        "x_train_downsampled = df_train_downsampled[\"line\"]\n",
        "y_train_downsampled = df_train_downsampled[\"speaker\"]"
      ],
      "metadata": {
        "id": "p80X8Ibuno6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_downsampled.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gsbKU7cgZ8j",
        "outputId": "8c2d9442-dde9-4485-925a-1cc0f422a1dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14888,)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split training data\n",
        "\n",
        "Here we split the training data so that we won't overfit when training the meta classifier."
      ],
      "metadata": {
        "id": "79FSzMGPHyfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_lg, x_train_sm, y_train_lg, y_train_sm = train_test_split(x_train_downsampled, y_train_downsampled, test_size=0.2, random_state=RAND_STATE)\n",
        "print(x_train_lg.shape)\n",
        "print(x_train_sm.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "878f690e-b860-42bb-ed61-1fcf84939e43",
        "id": "4a8WnD4xHyxW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13399,)\n",
            "(1489,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baselines"
      ],
      "metadata": {
        "id": "SVTNMBQlAuaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random baseline (stratified)\n",
        "\n",
        "Random classification according to class distribution."
      ],
      "metadata": {
        "id": "HFE8gxxTA4zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_clf = DummyClassifier(strategy=\"stratified\", random_state=RAND_STATE)\n",
        "\n",
        "preds = dummy_clf.fit(x_train, y_train).predict(x_test)\n",
        "\n",
        "evaluate_results(preds, y_test)\n"
      ],
      "metadata": {
        "id": "q7Cf0K0wA_Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Baseline 2\n",
        "\n",
        "This baseline always predict the most frequent class."
      ],
      "metadata": {
        "id": "vL2oOkE3CJyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = [\"Michael\"] * len(y_test)\n",
        "\n",
        "evaluate_results(preds, y_test)"
      ],
      "metadata": {
        "id": "AeBuSaTkCOdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numerical Features Classifier"
      ],
      "metadata": {
        "id": "mJXi3zamtwid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def get_sentiment(text):\n",
        "  return TextBlob(text).sentiment\n",
        "\n"
      ],
      "metadata": {
        "id": "q1LbkxTe2_cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create features"
      ],
      "metadata": {
        "id": "mWg6pFGU3r08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_processed_set(x):\n",
        "  line_lengths = pd.Series()\n",
        "  line_sentiments = pd.Series()\n",
        "  x_new = pd.DataFrame(columns=[\"length\", \"polarity\"])\n",
        "\n",
        "\n",
        "  for line in x:\n",
        "    line_lengths = line_lengths.append(pd.Series(len(line.split())))\n",
        "    line_sentiments = line_sentiments.append(pd.Series(get_sentiment(line)[0]))\n",
        "\n",
        "\n",
        "\n",
        "  x_new[\"length\"] = line_lengths\n",
        "  x_new[\"polarity\"] = line_sentiments\n",
        "\n",
        "  return x_new\n"
      ],
      "metadata": {
        "id": "driCzrEL3xnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_processed = get_processed_set(x_train_downsampled)\n",
        "x_train_processed_lg = get_processed_set(x_train_lg)\n",
        "x_test_processed = get_processed_set(x_test)"
      ],
      "metadata": {
        "id": "1V9uFpmD34aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot data points"
      ],
      "metadata": {
        "id": "cp8jMM_9Aa-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colors = {'Michael': 'b', 'Dwight': 'r', \"Jim\": 'purple', \"Pam\": \"green\"}\n",
        "\n",
        "\n",
        "plt.scatter(x_train_processed[\"length\"], x_train_processed[\"polarity\"],\n",
        "            color=[colors[speaker] for speaker in y_train_downsampled])"
      ],
      "metadata": {
        "id": "-u7K2tX256-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit and tune model"
      ],
      "metadata": {
        "id": "KxTKrSttAgda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    \"dual\": [False],\n",
        "    \"C\": [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
        "    \"solver\": [\"newton-cg\", \"sag\", \"saga\", \"lbfgs\" ],\n",
        "    \"n_jobs\": [-1],\n",
        "    \"random_state\": [RAND_STATE]\n",
        "}\n",
        "\n",
        "gs_len_sen_lg = GridSearchCV(LogisticRegression(), param_grid)\n",
        "gs_len_sen = GridSearchCV(LogisticRegression(), param_grid)\n",
        "\n",
        "\n",
        "gs_len_sen.fit(x_train_processed, y_train_downsampled)\n",
        "gs_len_sen_lg.fit(x_train_processed_lg, y_train_lg)\n",
        "\n",
        "print(\"best params: \", gs_len_sen.best_params_)\n",
        "print(\"best params: \", gs_len_sen_lg.best_params_)"
      ],
      "metadata": {
        "id": "wRnZGjrQ9TjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save trained model"
      ],
      "metadata": {
        "id": "IxmHjw1g3xgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_filename = \"gs_len_sen_10\"\n",
        "model_filename_lg = \"gs_len_sen_lg_10\"\n",
        "\n",
        "output = open(f'{file_path}/{model_filename}.pkl', 'wb')\n",
        "output_lg = open(f'{file_path}/{model_filename_lg}.pkl', 'wb')\n",
        "pickle.dump(gs_len_sen, output)\n",
        "pickle.dump(gs_len_sen_lg, output_lg)\n"
      ],
      "metadata": {
        "id": "DAZk5qc8Z34j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load trained model"
      ],
      "metadata": {
        "id": "M4bebYhZ30MZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_filename = \"gs_len_sen_10\"\n",
        "model_filename_lg = \"gs_len_sen_lg_10\"\n",
        "\n",
        "gs_len_sen = pickle.load(open(f\"{file_path}/{model_filename}.pkl\", 'rb'))\n",
        "gs_len_sen_lg = pickle.load(open(f\"{file_path}/{model_filename_lg}.pkl\", 'rb'))"
      ],
      "metadata": {
        "id": "POpt1Lfv3G73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate model"
      ],
      "metadata": {
        "id": "ot4B7Y88AjbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = gs_len_sen.predict(x_test_processed)\n",
        "preds2 = gs_len_sen_lg.predict(x_test_processed)\n",
        "\n",
        "print(\"preds with all downsampled data:\\n\")\n",
        "print(evaluate_results(preds, y_test))\n",
        "print(\"\\n\\npreds with subset of all downsampled data:\\n\")\n",
        "print(evaluate_results(preds2, y_test))"
      ],
      "metadata": {
        "id": "I4cRKJap_5Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f6mpKambfMdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TFIDF Classifier"
      ],
      "metadata": {
        "id": "GbowltO0trja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Perform Grid Search to tune hyperparameters"
      ],
      "metadata": {
        "id": "JeC6S7-Rt7jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    \"clf__dual\": [False],\n",
        "    \"clf__C\": [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1 ],\n",
        "    \"clf__solver\": [\"newton-cg\", \"sag\", \"saga\", \"lbfgs\" ],\n",
        "    \"clf__n_jobs\": [-1],\n",
        "    \"clf__random_state\": [RAND_STATE]\n",
        "}\n",
        "\n",
        "tfidf_lg_pipe = Pipeline([(\"TfidfVectorizer\", TfidfVectorizer()),\n",
        "                          (\"clf\", LogisticRegression())])\n",
        "\n",
        "\n",
        "tfidf_lg_pipe_lg = Pipeline([(\"TfidfVectorizer\", TfidfVectorizer()),\n",
        "                          (\"clf\", LogisticRegression())])\n",
        "\n",
        "gs_lg_tfidf = GridSearchCV(tfidf_lg_pipe, param_grid)\n",
        "gs_lg_tfidf_lg = GridSearchCV(tfidf_lg_pipe_lg, param_grid)\n",
        "\n",
        "gs_lg_tfidf.fit(x_train_downsampled, y_train_downsampled)\n",
        "gs_lg_tfidf_lg.fit(x_train_lg, y_train_lg)\n",
        "\n",
        "print(\"Best parameters: \", gs_lg_tfidf.best_params_)\n",
        "print(\"Best parameters: \", gs_lg_tfidf_lg.best_params_)"
      ],
      "metadata": {
        "id": "FDfbFyFxuH3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save trained model"
      ],
      "metadata": {
        "id": "DYHXiZZu344-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_filename = \"gs_lg_tfidf_10.pkl\"\n",
        "model_filename_lg = \"gs_lg_tfidf_lg_10.pkl\"\n",
        "\n",
        "output = open(f'{file_path}/{model_filename}', 'wb')\n",
        "output_lg = open(f'{file_path}/{model_filename_lg}', 'wb')\n",
        "pickle.dump(gs_lg_tfidf, output)\n",
        "pickle.dump(gs_lg_tfidf_lg, output_lg)"
      ],
      "metadata": {
        "id": "Zxodr1fp37Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load trained model"
      ],
      "metadata": {
        "id": "j1WKRGYy37Zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_filename = \"gs_lg_tfidf_10.pkl\"\n",
        "model_filename_lg = \"gs_lg_tfidf_lg_10.pkl\"\n",
        "\n",
        "gs_lg_tfidf =  pickle.load(open(f\"{file_path}/{model_filename}\", 'rb'))\n",
        "gs_lg_tfidf_lg = pickle.load(open(f\"{file_path}/{model_filename_lg}\", 'rb'))"
      ],
      "metadata": {
        "id": "zQMS2GAM5kD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate classifier"
      ],
      "metadata": {
        "id": "p_4gIxHP2Ybl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = gs_lg_tfidf.predict(x_test)\n",
        "preds2 = gs_lg_tfidf_lg.predict(x_test)\n",
        "\n",
        "print(\"Using all downsampled data:\\n\")\n",
        "print(\"Classification report:\\n\", classification_report(y_test, preds))\n",
        "print(\"Cohen Kappa score: \", cohen_kappa_score(y_test, preds))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix(y_test, preds))\n",
        "print(\"\\n\\n Confusion Matrix:\\n\")\n",
        "disp.plot()\n",
        "\n",
        "print(\"Using a subset of all downsampled data:\\n\")\n",
        "print(\"Classification report:\\n\", classification_report(y_test, preds2))\n",
        "print(\"Cohen Kappa score: \", cohen_kappa_score(y_test, preds2))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix(y_test, preds2))\n",
        "print(\"\\n\\n Confusion Matrix:\\n\")\n",
        "disp.plot()\n",
        "\n",
        "print(\"Classes to index mapping: \", gs_lg_tfidf.classes_)"
      ],
      "metadata": {
        "id": "UVJEV_PK0vBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error_indexes_tfidf = [index  for index, pred_tuple in enumerate(zip(preds, y_test)) if pred_tuple[0] != pred_tuple[1]]"
      ],
      "metadata": {
        "id": "kUGEbUxfxp00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT classifier"
      ],
      "metadata": {
        "id": "mfSkx0OLWgoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install simpletransformers\n",
        "!pip freeze | grep simpletransformers"
      ],
      "metadata": {
        "id": "7GsIAQ0TWmNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simpletransformers.classification import ClassificationModel\n",
        "import os\n",
        "\n",
        "random.seed(RAND_STATE)\n",
        "np.random.seed(seed=RAND_STATE)\n",
        "\n",
        "train_args = {\n",
        "    \"reprocess_input_data\": True,\n",
        "  \"fp16\": False,\n",
        "  \"num_train_epochs\": 4,\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"learning_rate\": 4e-5,\n",
        "    \"manual_seed\": RAND_STATE,\n",
        "    \"use_multiprocessing\": False,\n",
        "    \"use_multiprocessing_for_evaluation\": False\n",
        "}\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "bert_clf_lg = ClassificationModel(\"bert\", \"bert-base-uncased\", num_labels=4, args=train_args)\n",
        "bert_clf = ClassificationModel(\"bert\", \"bert-base-uncased\", num_labels=4, args=train_args)"
      ],
      "metadata": {
        "id": "kTvaaCe0WxBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = {\n",
        "    \"Michael\": 0,\n",
        "    \"Dwight\": 1,\n",
        "    \"Jim\": 2,\n",
        "    \"Pam\": 3\n",
        "}\n"
      ],
      "metadata": {
        "id": "d21yxpUVo6EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = {\n",
        "    \"Michael\": 0,\n",
        "    \"Dwight\": 1,\n",
        "    \"Jim\": 2,\n",
        "    \"Pam\": 3\n",
        "}\n",
        "\n",
        "# Map speaker strings to integers\n",
        "speakers = np.array([])\n",
        "for speaker in y_train_downsampled:\n",
        "  speakers = np.append(speakers, labels[speaker])\n",
        "\n",
        "speakers_lg = np.array([])\n",
        "for speaker in y_train_lg:\n",
        "  speakers_lg = np.append(speakers_lg, labels[speaker])\n",
        "\n",
        "df_train = pd.DataFrame(columns=[\"line\", \"speaker\"])\n",
        "df_train[\"line\"] = x_train_downsampled\n",
        "df_train[\"speaker\"] = speakers\n",
        "\n",
        "df_train_lg = pd.DataFrame(columns=[\"line\", \"speaker\"])\n",
        "df_train_lg[\"line\"] = x_train_lg\n",
        "df_train_lg[\"speaker\"] = speakers_lg\n"
      ],
      "metadata": {
        "id": "GlPGVG3iXIAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train bert model on all training data."
      ],
      "metadata": {
        "id": "39Nrk-7e7s9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "quw8oO7bqtI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(RAND_STATE)\n",
        "np.random.seed(seed=RAND_STATE)\n",
        "\n",
        "bert_clf.train_model(df_train)"
      ],
      "metadata": {
        "id": "Oj2K8GWU7jJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AzDFZnSmrDZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save model"
      ],
      "metadata": {
        "id": "Rduadw_y74rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_filename = \"bert_10.pkl\"\n",
        "output = open(f'{file_path}/{model_filename}', 'wb')\n",
        "pickle.dump(bert_clf, output)"
      ],
      "metadata": {
        "id": "yU7n2X_E8ACy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load model trained on all training data"
      ],
      "metadata": {
        "id": "jikvyR8DCe14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_filename = \"bert_10.pkl\"\n",
        "\n",
        "bert_clf =  pickle.load(open(f\"{file_path}/{model_filename}\", 'rb'))"
      ],
      "metadata": {
        "id": "rvSksqhVCiSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train bert model on subset of training data."
      ],
      "metadata": {
        "id": "j8Q4E6iL71SY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(RAND_STATE)\n",
        "np.random.seed(seed=RAND_STATE)\n",
        "\n",
        "bert_clf_lg.train_model(df_train_lg)"
      ],
      "metadata": {
        "id": "zKNE6WBY76sM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9xexyYMUbR-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save model"
      ],
      "metadata": {
        "id": "q-jarnOi79O0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_filename_lg = \"bert_lg_10.pkl\"\n",
        "output_lg = open(f'{file_path}/{model_filename_lg}', 'wb')\n",
        "pickle.dump(bert_clf_lg, output_lg)"
      ],
      "metadata": {
        "id": "_Ko5z2uG7ldD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load model trained on subset of training data."
      ],
      "metadata": {
        "id": "zPLf6JYvJY-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_filename_lg = \"bert_lg_10.pkl\"\n",
        "\n",
        "bert_clf_lg =  pickle.load(open(f\"{file_path}/{model_filename_lg}\", 'rb'))"
      ],
      "metadata": {
        "id": "cCGwCitKJd8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate on test data"
      ],
      "metadata": {
        "id": "jYOlPJKR9wuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(RAND_STATE)\n",
        "np.random.seed(seed=RAND_STATE)\n"
      ],
      "metadata": {
        "id": "VvGN4jSIX_G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "def f1_multiclass(labels, preds):\n",
        "    return f1_score(labels, preds, average='micro')"
      ],
      "metadata": {
        "id": "u6shGFUk7Dz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.DataFrame(columns=[\"line\", \"speaker\"])\n",
        "\n",
        "speakers = np.array([])\n",
        "for speaker in y_test:\n",
        "  speakers = np.append(speakers, labels[speaker])\n",
        "\n",
        "df_test[\"line\"] = x_test\n",
        "df_test[\"speaker\"] = speakers\n",
        "\n",
        "result, model_outputs, wrong_predictions = bert_clf_lg.eval_model(df_test, f1=f1_multiclass, acc=accuracy_score)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8LAdfIzgbe-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "for output in model_outputs:\n",
        "    preds.append(output.argmax())  "
      ],
      "metadata": {
        "id": "w8Y1cKO5bmhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, cohen_kappa_score\n",
        "\n",
        "print(f\"Result: {result}\")\n",
        "print(\"Evaluation report:\\n\", classification_report(list(df_test[\"speaker\"]), preds))\n",
        "print(\"\\nCohen Kappa score = \", cohen_kappa_score(list(df_test[\"speaker\"]), preds))"
      ],
      "metadata": {
        "id": "z28DB3KqbqZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(RAND_STATE)\n",
        "np.random.seed(seed=RAND_STATE)\n",
        "\n",
        "error_indexes_bert = [index  for index, pred_tuple in enumerate(zip(preds, list(df_test[\"speaker\"]))) if pred_tuple[0] != pred_tuple[1]]\n",
        "\n",
        "print(\"intersect bert and tfidf error indexes: \", len(set(error_indexes_bert).intersection(set(error_indexes_tfidf))))\n",
        "print(\"total error len tfidif: \", len(error_indexes_tfidf))\n",
        "print(\"total error len BERT: \", len(error_indexes_bert))\n",
        "print(\"---------------- 20 random missclassified lines ----------------\")\n",
        "indexes = list(set(error_indexes_bert).intersection(set(error_indexes_tfidf)))\n",
        "random.shuffle(indexes)\n",
        "missclassified_lines = [(x_test.iloc[i], y_test.iloc[i]) for i in indexes[:20]]\n",
        "\n",
        "print(missclassified_lines)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uEEukmRzyi5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensamble classifier"
      ],
      "metadata": {
        "id": "qwpAvAl-sYJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "class EnsambleClassifier:\n",
        "  \"\"\"\n",
        "  This class trains the meta classifier of this ensamble. It takes three\n",
        "  different classifiers, which are all pre-trained. It uses the probability output\n",
        "  from each of these to perform a classification using logistic regression.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_clf, tfidf_clf, bert_clf, meta_clf):\n",
        "    self.num_clf = num_clf\n",
        "    self.tfidf_clf = tfidf_clf\n",
        "    self.bert_clf = bert_clf\n",
        "\n",
        "    param_grid = {\n",
        "      \"dual\": [False],\n",
        "      \"C\": [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
        "      \"solver\": [\"newton-cg\", \"sag\", \"saga\", \"lbfgs\" ],\n",
        "      \"n_jobs\": [-1],\n",
        "      \"random_state\": [RAND_STATE]\n",
        "    }\n",
        "\n",
        "    self.gs_meta_clf = GridSearchCV(meta_clf, param_grid=param_grid)\n",
        "\n",
        "  def set_base_classifiers(self, classifiers):\n",
        "    self.num_clf = classifiers[0]\n",
        "    self.tfidf_clf = classifiers[1]\n",
        "    self.bert_clf = classifiers[2]\n",
        "\n",
        "  def train(self, x_train, y_train):\n",
        "    df_x = self.__get_base_class_probs(x_train)\n",
        "    \n",
        "    self.gs_meta_clf.fit(df_x, y_train)\n",
        "    print(\"Best meta classifier params: \", self.gs_meta_clf.best_params_)\n",
        "    print(\"Meta classifier hyperparameter tuning and training done.\")\n",
        "\n",
        "  def predict(self, x):\n",
        "    df_x = self.__get_base_class_probs(x)\n",
        "    \n",
        "    # Get final predictions from meta classifier\n",
        "    self.preds = self.gs_meta_clf.predict(df_x)\n",
        "    return self.preds\n",
        "\n",
        "  def __get_base_class_probs(self, x):\n",
        "    # Create special features\n",
        "    x_processed = get_processed_set(x)\n",
        "\n",
        "    # Get probabilities from all base classifiers\n",
        "    probs_num = self.num_clf.predict_proba(x_processed)\n",
        "    probs_tfidf = self.tfidf_clf.predict_proba(x)\n",
        "    predictions, raw_output = self.bert_clf.predict(list(x))\n",
        "    probs_bert = []\n",
        "    for row in raw_output:\n",
        "      probs_bert.append(softmax(row))\n",
        "\n",
        "\n",
        "    # Create new x matrix with all probabilities as features\n",
        "    df_x = self.__get_df_with_proba_features(probs_num, probs_tfidf, probs_bert)\n",
        "\n",
        "    return df_x\n",
        "\n",
        "  def __get_df_with_proba_features(self, probs_num, probs_tfidf, probs_bert):\n",
        "\n",
        "    x_with_proba = pd.DataFrame(columns=[\"D1\", \"J1\", \"M1\", \"P1\",\n",
        "                                         \"D2\", \"J2\", \"M2\", \"P2\", \n",
        "                                         \"D3\", \"J3\", \"M3\", \"P3\"])\n",
        "                                         #\"line_len\", \"polarity\"])\n",
        "    for i in range(len(probs_tfidf)):\n",
        "      new_row = {\n",
        "              \"D1\": probs_tfidf[i][0],\n",
        "              \"J1\": probs_tfidf[i][1],\n",
        "              \"M1\": probs_tfidf[i][2],\n",
        "              \"P1\": probs_tfidf[i][3],\n",
        "              \"D2\": probs_num[i][0],\n",
        "              \"J2\": probs_num[i][1],\n",
        "              \"M2\": probs_num[i][2],\n",
        "              \"P2\": probs_num[i][3],\n",
        "              \"D3\": probs_bert[i][1],\n",
        "              \"J3\": probs_bert[i][2],\n",
        "              \"M3\": probs_bert[i][0],\n",
        "              \"P3\": probs_bert[i][3],\n",
        "              #\"line_len\": x_processed.iloc[i][\"length\"],\n",
        "              #\"polarity\": x_processed.iloc[i][\"polarity\"]\n",
        "               }\n",
        "\n",
        "      x_with_proba = x_with_proba.append(new_row, ignore_index=True)\n",
        "\n",
        "    return x_with_proba\n"
      ],
      "metadata": {
        "id": "7FDz1OcYsjQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(RAND_STATE)\n",
        "np.random.seed(seed=RAND_STATE)\n",
        "\n",
        "clf = EnsambleClassifier(gs_len_sen_lg, gs_lg_tfidf_lg, bert_clf_lg, LogisticRegression())\n",
        "\n",
        "clf.train(x_train_sm, y_train_sm)\n"
      ],
      "metadata": {
        "id": "1kmhsTSn_8gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.set_base_classifiers([gs_len_sen, gs_lg_tfidf, bert_clf])"
      ],
      "metadata": {
        "id": "QLVVudAdWhbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = clf.predict(x_test)"
      ],
      "metadata": {
        "id": "YrnDOE8OObe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results"
      ],
      "metadata": {
        "id": "3HGKyXnFTbDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"preds:\\n\", preds)\n",
        "\n",
        "print(\"Classification report:\\n\", classification_report(y_test, preds))\n",
        "print(\"\\nCohen Kappa score = \", cohen_kappa_score(y_test, preds))"
      ],
      "metadata": {
        "id": "xBRW_F4qBjrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Error analysis on results"
      ],
      "metadata": {
        "id": "_8auBtg3jzlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "random.seed(RAND_STATE)\n",
        "np.random.seed(seed=RAND_STATE)\n",
        "\n",
        "error_indexes = [index  for index, pred_tuple in enumerate(zip(preds, y_test)) if pred_tuple[0] != pred_tuple[1]]\n",
        "correct_indexes = [index  for index, pred_tuple in enumerate(zip(preds, y_test)) if pred_tuple[0] == pred_tuple[1]]\n",
        "random.shuffle(error_indexes)\n",
        "print(\"---------------- 20 random missclassified lines ----------------\")\n",
        "missclassified_lines = [(x_test.iloc[i], y_test.iloc[i], preds[i]) for i in error_indexes[:21]]\n",
        "print(missclassified_lines)\n",
        "\n",
        "print(\"---------------- 20 random correctly classified lines ----------\")\n",
        "\n",
        "correct_lines = [(x_test.iloc[i], y_test.iloc[i]) for i in correct_indexes[:20]]\n",
        "print(correct_lines)\n",
        "\n",
        "line_lens_missclassified = [len(x_test.iloc[index]) for index in error_indexes]\n",
        "avg_line_missclassified = sum(line_lens_missclassified)/len(line_lens_missclassified)\n",
        "\n",
        "line_lens_correct = [len(x_test.iloc[index]) for index in correct_indexes]\n",
        "avg_line_correct = sum(line_lens_correct)/len(line_lens_correct)\n",
        "\n",
        "print(\"Average line length of missclassified: \", avg_line_missclassified)\n",
        "print(\"Average line length of correctly classified: \", avg_line_correct)\n",
        "line_lens_missclassified.sort(reverse=True)\n",
        "line_lens_correct.sort(reverse=True)\n",
        "\n",
        "print(line_lens_missclassified)\n",
        "print(line_lens_correct)\n"
      ],
      "metadata": {
        "id": "SbAt2UP0j5K5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}